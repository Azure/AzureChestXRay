{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "##### Copyright (C) Microsoft Corporation.  \n",
    "see license file for details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow multiple displays per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/azureml-share/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AZUREML_NATIVE_SHARE_DIRECTORY mapping to host dir is set by _nativeSharedDirectory_ in .compute file \n",
    "\n",
    "import os\n",
    "try:\n",
    "    amlWBSharedDir = os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']    \n",
    "except:\n",
    "    amlWBSharedDir = ''\n",
    "    print('not using aml services?')\n",
    "    \n",
    "amlWBSharedDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import utlity functions\n",
    "\n",
    "import sys, os\n",
    "paths_to_append = [os.path.join(os.getcwd(), os.path.join(*(['Code',  'src'])))]\n",
    "def add_path_to_sys_path(path_to_append):\n",
    "    if not (any(path_to_append in paths for paths in sys.path)):\n",
    "        sys.path.append(path_to_append)\n",
    "\n",
    "[add_path_to_sys_path(crt_path) for crt_path in paths_to_append]\n",
    "\n",
    "import azure_chestxray_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/azureml-share/chestxray/data/ChestX-ray8'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/azureml-share/chestxray/output'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create base directories for the file path variables \n",
    "# paths are tipically container level dirs mapped to a host dir for data persistence.\n",
    "\n",
    "prj_consts = azure_chestxray_utils.chestxray_consts()\n",
    "\n",
    "data_base_input_dir=os.path.join(amlWBSharedDir, os.path.join(*(prj_consts.BASE_INPUT_DIR_list)))\n",
    "data_base_output_dir=os.path.join(amlWBSharedDir, os.path.join(*(prj_consts.BASE_OUTPUT_DIR_list)))  \n",
    "\n",
    "data_base_input_dir\n",
    "data_base_output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/azureml-share/chestxray/data/ChestX-ray8/ChestXray-NIHCC'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig images number:['112120'] \n"
     ]
    }
   ],
   "source": [
    "# chest xray images are in nih_chest_xray_data_dir\n",
    "nih_chest_xray_data_dir=os.path.join(data_base_input_dir, \n",
    "                                     os.path.join(*(prj_consts.ChestXray_IMAGES_DIR_list)))\n",
    "nih_chest_xray_data_dir\n",
    "\n",
    "# check if we have all 112120 images in nih_chest_xray_data_dir\n",
    "orig_images_no = !find $nih_chest_xray_data_dir -type f | wc -l\n",
    "print(\"orig images number:{} \".format(orig_images_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/azureml-share/chestxray/data/ChestX-ray8/ChestXray-NIHCC_other'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBox_List_2017.csv  Data_Entry_2017.csv  blacklist.csv\r\n"
     ]
    }
   ],
   "source": [
    "# check if we have patients file list Data_Entry_2017.csv and BBox_List_2017.csv (https://nihcc.app.box.com/v/ChestXray-NIHCC)\n",
    "# blacklist.csv is genrated by data scientists with no medical background\n",
    "\n",
    "other_data_dir=os.path.join(data_base_input_dir, os.path.join(*(prj_consts.ChestXray_OTHER_DATA_DIR_list)))\n",
    "other_data_dir\n",
    "# !mkdir -p {other_data_dir}\n",
    "!ls $other_data_dir\n",
    "\n",
    "# data is split into train/test/validation partitions\n",
    "data_partitions_dir=os.path.join(data_base_output_dir, os.path.join(*(prj_consts.DATA_PARTITIONS_DIR_list)))  \n",
    "!mkdir -p {data_partitions_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Validation/Test Data partitioning \n",
    " - remove the images in the blacklist.csv where the image has low quality. \n",
    " - remove the NIH bounding box patients since we will save those patients for later validation use. \n",
    " - We will also divide data into train/valid/test dataset using a 7:1:2 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of original patient id is 30805\n",
      "len of cleaned patient id is 30079\n",
      "len of unique patient id with annotated data 726\n",
      "len of patient id with annotated data 984\n"
     ]
    }
   ],
   "source": [
    "# remove NIH manually annotated data (groung truth with heavy pathologies, no healthy patients) \n",
    "# exclude what visusally looks like bad images to data scientists with no medical background\n",
    "# todo\n",
    "# This should prob be a generic function\n",
    "\n",
    "\n",
    "total_patient_number = 30805\n",
    "NIH_annotated_file = 'BBox_List_2017.csv' # exclude from train pathology annotated by radiologists \n",
    "manually_selected_bad_images_file = 'blacklist.csv'# exclude what viusally looks like bad images\n",
    "\n",
    "patient_id_original = [i for i in range(1,total_patient_number + 1)]\n",
    "\n",
    "# ignored images list is used later, since this is not a patient ID level issue\n",
    "ignored_images_set = set()\n",
    "with open(os.path.join(other_data_dir, manually_selected_bad_images_file), 'r') as f:\n",
    "    for line in f:\n",
    "        # delete the last char which is \\n\n",
    "        ignored_images_set.add(line[:-1])\n",
    "        if int(line[:-9]) >= 30805:\n",
    "            print(line[:-1])\n",
    "\n",
    "bbox_df = pd.read_csv(os.path.join(other_data_dir, NIH_annotated_file))\n",
    "bbox_patient_index_df = bbox_df['Image Index'].str.slice(3, 8)\n",
    "\n",
    "bbox_patient_index_list = []\n",
    "for index, item in bbox_patient_index_df.iteritems():\n",
    "    bbox_patient_index_list.append(int(item))\n",
    "\n",
    "patient_id = list(set(patient_id_original) - set(bbox_patient_index_list))\n",
    "print(\"len of original patient id is\", len(patient_id_original))\n",
    "print(\"len of cleaned patient id is\", len(patient_id))\n",
    "print(\"len of unique patient id with annotated data\", \n",
    "      len(list(set(bbox_patient_index_list))))\n",
    "print(\"len of patient id with annotated data\",bbox_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first ten patient ids are [24303, 16035, 4967, 28624, 5378, 20335, 17069, 12271, 16975, 4469]\n",
      "train:21563 valid:3081 test:6161\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "random.shuffle(patient_id)\n",
    "\n",
    "print(\"first ten patient ids are\", patient_id[:10])\n",
    "\n",
    "# training:valid:test=7:1:2\n",
    "patient_id_train = patient_id[:int(total_patient_number * 0.7)]\n",
    "patient_id_valid = patient_id[int(total_patient_number * 0.7):int(total_patient_number * 0.8)]\n",
    "# get the rest of the patient_id as the test set\n",
    "patient_id_test = patient_id[int(total_patient_number * 0.8):]\n",
    "patient_id_test.extend(bbox_patient_index_list)\n",
    "patient_id_test = list(set(patient_id_test))\n",
    "\n",
    "\n",
    "print(\"train:{} valid:{} test:{}\".format(len(patient_id_train), len(patient_id_valid), len(patient_id_test)))\n",
    "\n",
    "# test_set = test_set+left_out_patient_id\n",
    "# print(\"train:{} valid:{} test:{}\".format(len(train_set), len(valid_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a few more project constants\n",
    "\n",
    "pathologies_name_list = prj_consts.DISEASE_list\n",
    "NIH_patients_and_labels_file = 'Data_Entry_2017.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally do preprocessing\n",
    "Save labels and partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(os.path.join(other_data_dir, NIH_patients_and_labels_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(current_df, patient_ids):\n",
    "    image_name_index = []\n",
    "    image_labels = {}\n",
    "    for individual_patient in tqdm.tqdm(patient_ids):\n",
    "        for _, row in current_df[current_df['Patient ID'] == individual_patient].iterrows():\n",
    "            processed_image_name = row['Image Index']\n",
    "            if processed_image_name in ignored_images_set:\n",
    "                pass\n",
    "            else:\n",
    "                image_name_index.append(processed_image_name)\n",
    "                image_labels[processed_image_name] = np.zeros(14, dtype=np.uint8)\n",
    "                for disease_index, ele in enumerate(pathologies_name_list):\n",
    "                    if re.search(ele, row['Finding Labels'], re.IGNORECASE):\n",
    "                        image_labels[processed_image_name][disease_index] = 1\n",
    "                    else:\n",
    "                        # redundant code but just to make it more readable\n",
    "                        image_labels[processed_image_name][disease_index] = 0\n",
    "                # print(\"processed\", row['Image Index'])\n",
    "    return image_name_index, image_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21563/21563 [00:35<00:00, 606.57it/s]\n",
      "100%|██████████| 3081/3081 [00:05<00:00, 614.10it/s]\n",
      "100%|██████████| 6161/6161 [00:13<00:00, 449.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, valid, test image number is: 68508 9495 32893\n"
     ]
    }
   ],
   "source": [
    "# # create and save train/test/validation partitions list\n",
    "\n",
    "train_data_index, train_labels = process_data(labels_df, patient_id_train)\n",
    "valid_data_index, valid_labels = process_data(labels_df, patient_id_valid)\n",
    "test_data_index, test_labels = process_data(labels_df, patient_id_test)\n",
    "\n",
    "print(\"train, valid, test image number is:\", len(train_data_index), len(valid_data_index), len(test_data_index))\n",
    "\n",
    "# save the data\n",
    "labels_all = {}\n",
    "labels_all.update(train_labels)\n",
    "labels_all.update(valid_labels)\n",
    "labels_all.update(test_labels)\n",
    "\n",
    "partition_dict = {'train': train_data_index, 'test': test_data_index, 'valid': valid_data_index}\n",
    "\n",
    "with open(os.path.join(data_partitions_dir,'labels14_unormalized_cleaned.pickle'), 'wb') as f:\n",
    "    pickle.dump(labels_all, f)\n",
    "\n",
    "with open(os.path.join(data_partitions_dir,'partition14_unormalized_cleaned.pickle'), 'wb') as f:\n",
    "    pickle.dump(partition_dict, f)\n",
    "    \n",
    "# also save the patient id partitions for pytorch training    \n",
    "with open(os.path.join(data_partitions_dir,'train_test_valid_data_partitions.pickle'), 'wb') as f:\n",
    "    pickle.dump([patient_id_train,patient_id_valid,\n",
    "                 patient_id_test,\n",
    "                list(set(bbox_patient_index_list))], f)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'00001256_000.png': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " '00010535_020.png': array([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " '00017170_004.png': array([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " '00017906_025.png': array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8),\n",
       " '00030353_000.png': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check, see train labels\n",
    "\n",
    "type(train_labels)\n",
    "{k: train_labels[k] for k in list(train_labels)[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter nbconvert --to html .\\Code\\02_Model\\000_preprocess.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
